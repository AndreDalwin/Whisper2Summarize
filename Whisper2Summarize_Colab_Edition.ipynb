{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6A88HNoShgQTgXIr2QBI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreDalwin/Whisper2Summarize/blob/main/Whisper2Summarize_Colab_Edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper2Summarize: Colab Edition\n",
        "\n",
        "This is a modified version of [Whisper2Summarize](https://github.com/AndreDalwin/Whisper2Summarize) that works in Google Colab.\n",
        "\n",
        "To Begin, you need to initialize the requirements in order to run the program"
      ],
      "metadata": {
        "id": "UUYYr_ArWcJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3CCY-m4Wbo6"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/openai/whisper.git openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The program then needs to be initialized. Ensure you place in your audio file name, select Whisper Model to use, and your OpenAI API Key (Don't worry. Google Colab doesn't save your apikey if you are in Playground Mode.)\n",
        "\n"
      ],
      "metadata": {
        "id": "yjL9OR_GXKCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  \n",
        "import whisper \n",
        "import openai\n",
        "import tqdm\n",
        "import sys\n",
        "\n",
        "audio = \"audio.mp3\" #Make sure you upload the audio file (mp3,wav,m4a) into the session storage!\n",
        "model = \"base\" #possible options are 'tiny', 'base', 'small', 'medium', and 'large'\n",
        "apikey = \"INSERT API KEY HERE\""
      ],
      "metadata": {
        "id": "bPeusbKrW6so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will be setting up the transcribe() and gpt_process() functions."
      ],
      "metadata": {
        "id": "vAH0-g53YUvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe(audio,model_type):\n",
        "    class _CustomProgressBar(tqdm.tqdm):\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self._current = self.n  \n",
        "            \n",
        "        def update(self, n):\n",
        "            super().update(n)\n",
        "            self._current += n\n",
        "            \n",
        "            print(\"Audio Transcribe Progress: \" + self._current +\"/\" +self.total)\n",
        "            \n",
        "    devices = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "    model = whisper.load_model(model_type, device = devices)\n",
        "    transcribe_module = sys.modules['whisper.transcribe']\n",
        "    transcribe_module.tqdm.tqdm = _CustomProgressBar\n",
        "\n",
        "    print(\"Beginning Transcribing Process...\")\n",
        "    result = model.transcribe(audio, verbose=None, fp16=False)\n",
        "    transcribed = result[\"text\"]\n",
        "    with open(\"Transcript.txt\", \"w\",encoding='utf-8') as text_file:\n",
        "        text_file.write(transcribed)\n",
        "        print(\"Saved Transcript to Transcript.txt\")\n",
        "    return transcribed\n",
        "\n",
        "def gpt_process(transcript):\n",
        "    openai.api_key = apikey\n",
        "    print(\"Processing Transcript with GPT...\")\n",
        "    n=1300\n",
        "    split = transcript.split()\n",
        "    snippet= [' '.join(split[i:i+n]) for i in range(0,len(split),n)]\n",
        "    ## For managing token limit\n",
        "    summary=\"\"\n",
        "    previous=\"\"\n",
        "    for i in range(0, len(snippet), 1):\n",
        "        print(\"Summarizing Transcribed Snippet {} of {}\".format(i+1,len(snippet)))\n",
        "        gpt_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"\\\"\" + snippet[i] + \"\\\"\\n Rewrite the transcript above into notes. Do not summarize and keep every information. For additional context here is the previous rewritten message: \\n \" + previous }],\n",
        "            temperature = 0.6,\n",
        "        )\n",
        "        previous = gpt_response['choices'][0]['message']['content']\n",
        "        summary += gpt_response['choices'][0]['message']['content']\n",
        "\n",
        "    with open(\"Summary.txt\", \"w\",encoding='utf-8') as text_file:\n",
        "        text_file.write(summary)\n",
        "        print(\"Summarizing Completed.\")\n",
        "        print(\"Saved Summary to Summary.txt\")\n"
      ],
      "metadata": {
        "id": "T9ugGhpuX9Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last, we will run the code. This will output 2 files into the session storage. The Transcript.txt as well as a Summary.txt"
      ],
      "metadata": {
        "id": "DJ--2-K0YbFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = transcribe(audio,model)\n",
        "gpt_process(text)"
      ],
      "metadata": {
        "id": "MlAigRLFYdpb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
